{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS396 Milestone.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"uSwrgAGI7I3h"},"source":["import json\n","import pandas as pd\n","import datetime\n","import scipy.stats\n","import numpy as np\n","import re\n","import seaborn as sb\n","import math\n","from scipy.spatial import distance\n","from textblob import TextBlob"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F7ppHu2u_0Ut"},"source":["# load data\n","# did not have any luck uploading the review and user json file\n","path = '/content/YelpDataset/'\n","business_df = pd.read_json(path+'yelp_academic_dataset_business.json', lines=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQPGq1u97Yx1"},"source":["# pre-processing -- May -- DONE\n","# Generated two python dataframe: \n","# 1) business_tokeep contains all the businesses with >2yr review history, total len = 141613\n","# 2) business_pan_eligible contains all businesses with >2yr review history even not counting ones after pandemic started, \n","    # with the pandemic start date set as 2020-03-01, total len = 138023"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PbFaQYmYaRDb"},"source":["# I loaded the review_df locally\n","review_df['new_date'] = pd.to_datetime(review_df.date, errors='coerce')\n","review_newest = review_df[['business_id', 'new_date']].groupby(['business_id']).max()\n","review_oldest = review_df[['business_id', 'new_date']].groupby(['business_id']).min()\n","review_duration = review_oldest.join(review_newest, on='business_id', lsuffix='_old', rsuffix='_new')\n","review_duration['duration'] = review_duration.new_date_new - review_duration.new_date_old\n","\n","# filter businesses based on review history\n","business_tokeep = review_duration[review_duration['duration']>datetime.timedelta(days=365*2)]\n","\n","# further filter businesses to only keep ones with enough history if not counting ones after pandemic started\n","pan_start = datetime.datetime.fromisoformat('2020-03-01')\n","business_pan_eligible = business_tokeep.assign(pan_duration = pan_start - business_tokeep['new_date_old'])\n","business_pan_eligible = business_pan_eligible[business_pan_eligible['pan_duration'] > datetime.timedelta(days=365*2)]\n","business_pan_eligible = business_pan_eligible.assign(duration_toUse = business_pan_eligible[['pan_duration', 'duration']].min(axis=1))\n","\n","# convert business_pan_eligible.duration_toUse to the unit of year\n","business_pan_eligible['duration_toUse'] = business_pan_eligible['duration_toUse'].dt.days.astype('int16')/365"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2kg0kashpHq3"},"source":["# histogram of businesses review duration\n","fig, (ax1, ax2) = plt.subplots(1, 2) \n","fig.suptitle('Histogram of businesses review history duration', fontsize = 24)\n","ax1.hist(business_pan_eligible['duration_toUse'], bins=15)\n","ax1.set_xlabel('Years', fontsize = 18)\n","ax1.set_ylabel('Count of businesses', fontsize = 18)\n","\n","ax2.hist(business_pan_eligible['duration_toUse'], bins=15, density=True, cumulative=True)\n","ax2.set_xlabel('Years', fontsize = 18)\n","ax2.set_ylabel('Cumulative ratio of businesses', fontsize = 18)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"si-u9i9kF95b"},"source":["My interpretation of this histogram: a pretty uniform distribution for businesses with review histories < 10 years. Only a selective few (<5%) businesses have a review history of >10 years"]},{"cell_type":"code","metadata":{"id":"FLJl-1lppHcE"},"source":["# based on the business_id from the business_pan_eligible, filter the review_df\n","# also filter out reviews made after 2020-03-01\n","selected_review = review_df[(review_df[\"business_id\"].isin(business_pan_eligible['business_id'])) & \n","                           (review_df['date'] < '2020-03-01')]\n","#len(selected_review) = 7696848\n","\n","# count number of reviews received by each business from the selected reviews\n","review_count = selected_review.groupby('business_id').count()\n","\n","# histogram of the number of reviews received by businesses\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 8]) \n","fig.suptitle('Histogram of review count for selected businesses', fontsize = 24)\n","ax1.hist(review_count['review_id'], bins=30)\n","ax1.set_xlabel('Number of reviews received by businesses', fontsize = 18)\n","ax1.set_ylabel('Count of businesses', fontsize = 18)\n","\n","ax2.hist(review_count['review_id'], bins=30, density=True, histtype='step', cumulative=True)\n","ax2.set_xlabel('Number of reviews received by businesses', fontsize = 18)\n","ax2.set_xscale('log')\n","ax2.set_yscale('log')\n","ax2.set_ylabel('Cumulative ratio of businesses', fontsize = 18)\n","\n","plt.show()\n","\n","\n","print('the 90th quantile of review count is: ',\n","      review_count['review_id'].quantile(0.9))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sJXJ13BWG6eV"},"source":["My interpretation of this histogram: over 97% businesses have less than 500 reviews accumulated over the entire duration of yelp history. Over 90% of businesses that fit our selective criteria have less than 129 reviews."]},{"cell_type":"code","metadata":{"id":"YgWlanChJeXV"},"source":["# just for fun, let's fit it with powerlaw and see what's the fitted xmin value\n","import powerLaw\n","count = review_count.review_id\n","fitted = powerlaw.Fit(count)\n","fitted.xmin"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kXVZ919uKBxL"},"source":["xmin returned here is 424, which makes sense based on the histogram plotted above. If we filter out businesses with more than 424 reviews received, then we will have 2544 businesses to work with. This is something to keep in mind. Currently I pre-processed the review dataset by selecting businesses with in the 90th percentile of review count."]},{"cell_type":"code","metadata":{"id":"NL_0UX4Y8aDg"},"source":["# further pre-processing: only select businesses with more than 129 reviews accumulated\n","# take businesses with top 10% review counts\n","# this will still give us 13814 businesses to analyze\n","\n","further_filtered_business = review_count[review_count['review_id'] >= \n","                                         review_count['review_id'].quantile(0.9)].reset_index()\n","\n","# filter the review dataframe accordingly\n","further_filtered_reviews = selected_review[selected_review[\"business_id\"].isin(\n","    further_filtered_business['business_id'])]\n","\n","# save the further_filtered_review into pickle\n","further_filtered_reviews.to_pickle('preprocessed_review.pkl')\n","\n","# groupby and count reviews received by businesses\n","review_count_new = further_filtered_reviews.groupby('business_id').count()\n","\n","# now plot the distribution of number of reviews received by businesses after pre-processing\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 8]) \n","fig.suptitle('Histogram of review count for businesses after pre-processing', fontsize = 24)\n","ax1.hist(review_count_new['review_id'], bins=20)\n","ax1.set_xlabel('Number of reviews received by businesses', fontsize = 18)\n","ax1.set_ylabel('Count of businesses', fontsize = 18)\n","ax1.set_xscale('log')\n","ax1.set_yscale('log')\n","\n","ax2.hist(review_count_new['review_id'], bins=20, density=True, cumulative=True, histtype='step')\n","ax2.set_xlabel('Number of reviews received by businesses', fontsize = 18)\n","ax2.set_ylabel('Cumulative atio of businesses', fontsize = 18)\n","ax2.set_xscale('log')\n","ax2.set_yscale('log')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"evt3TYn5JPDH"},"source":["My interpretation of this plot: still a little bit skewed, which is understandable, since the fitted xmin returned by the powerlaw function is 424, above our cutoff point."]},{"cell_type":"code","metadata":{"id":"fhHURQ0g-Vel"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DfHNim5-NmNT"},"source":["Let's say that we have saved a subset of reviews_df, based on whichever filtering criteria we eventually decide on.\n","\n","This variable was called further_filtered_reviews and was saved into a pickle file named 'preprocessed_review.pkl'"]},{"cell_type":"code","metadata":{"id":"DvINyHOzNGes"},"source":["# Let's pick up from this\n","preprocessed_review = pd.read_pickle('preprocessed_review.pkl')\n","# group by business_id and then sort the reviews based on date\n","# set business_id as index for easier computation later on\n","preprocessed_review = preprocessed_review.sort_values(by=['business_id','date']).set_index('business_id')\n","\n","# change the date to TimeDate object\n","preprocessed_review['date'] = pd.to_datetime(preprocessed_review['date'])\n","\n","# group by business, and then calculate how long ago each review was made \n","# relative to the most-recent one. Result returned as a timedelta object\n","preprocessed_review_delta = preprocessed_review.assign(delta = preprocessed_review[['date']]\n","                                                       .groupby('business_id')\n","                                                       .apply(lambda x: x - x.max()))\n","\n","# convert the timedelta object calculated into int, in the unit of days\n","preprocessed_review_delta['delta'] = subset_review_delta['delta'].dt.days"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5f-odeF8ZyL"},"source":["# function to segment by date -- May -- Done\n","def summarize_column_by_period(dataframe, col, timespan=1, unit='month', count=True):\n","    \"\"\"\n","    this function will call the timedelta_period_convert function\n","    dataframe: pandas dataframe containing reviews information, \n","               should have business_id as its index, and delta in int already\n","    \n","    col: str - column name to get summarized info from, e.g. 'stars', 'useful'\n","             will generated columns for mean, max, min, and sd\n","    \n","    timespan: see doc for timedelta_period_convert function\n","    unit: see doc for timedelta_period_convert function\n","    count: summarize the number of reviews received during this time frame; default is True\n","    \n","    \n","    RETURN:\n","    summarized_df: Pandas dataframe with business_id and period as index, and summarized information\n","    \"\"\"\n","    \n","    df_period = dataframe.assign(period = timedelta_period_convert(\n","        dataframe['delta'], timespan=timespan, unit=unit))\n","    \n","    if count:\n","        df_period_summarized = df_period.groupby(['business_id', 'period']).agg(\n","            mean = pd.NamedAgg(column=col, aggfunc=np.mean),\n","            sd = pd.NamedAgg(column=col, aggfunc=np.std),\n","            minimum = pd.NamedAgg(column=col, aggfunc=min),\n","            maximum = pd.NamedAgg(column=col, aggfunc=max),\n","            review_count = pd.NamedAgg(column='review_id', aggfunc=len)\n","            )\n","    else:\n","        df_period_summarized = df_period.groupby(['business_id', 'period']).agg(\n","            mean = pd.NamedAgg(column=col, aggfunc=np.mean),\n","            sd = pd.NamedAgg(column=col, aggfunc=np.std),\n","            minimum = pd.NamedAgg(column=col, aggfunc=min),\n","            maximum = pd.NamedAgg(column=col, aggfunc=max)\n","            )\n","    \n","    return df_period_summarized\n","\n","\n","def timedelta_period_convert(time_series, timespan = 1, unit = 'month'):\n","    \"\"\"\n","    time_series: the delta column of the dataframe - note: needs to be converted to int already\n","    timespan: an integer, if we want to segment by 3 month, then timespan would be 3\n","    unit: 'month', 'week', 'days', 'year'\n","    \n","    RETURN:\n","    new_period: a Pandas Series\n","    \"\"\"\n","    unit_dict = {'month':30, 'week': 7, 'days': 1, 'year': 365}\n","    days = timespan * unit_dict[unit]\n","    \n","    new_period = np.floor(time_series/days)\n","    \n","    return new_period\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dACohFT-PyM6"},"source":["For proof of concept, I tried my function on a subset of our filtered review dataframe. To be exact, I tried this on a test dataframe containing reviews from two businesses.\n","\n","See code below for more info"]},{"cell_type":"code","metadata":{"id":"kJzAcvmZPxl8"},"source":["test_set = {'zzin1d1oHi81GuI0ufo1VA', '--164t1nclzzmca7eDiJMw'}\n","test_reviews = subset_review[subset_review[\"business_id\"].isin(test_set)]\n","test_reviews = test_reviews.sort_values(by=['business_id','date']).set_index('business_id')\n","\n","test_reviews['date'] = pd.to_datetime(test_reviews['date'])\n","\n","test_review_delta = test_reviews.assign(delta = test_reviews[['date']].groupby('business_id').apply(\n","lambda x: x - x.max()))\n","\n","test_review_delta['delta'] = test_review_delta['delta'].dt.days\n","summarize_column_by_period(test_review_delta, 'stars', timespan=6, unit='month')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AM4hrfQSPxWh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_he78jGr8Zo2"},"source":["## trend analysis (as a function). -- Sophia will do this \n","def calculate_regression(dataframe):\n","    \"\"\"\n","    calculates r^2 value of each business over its respective time period\n","    dataframe: resulting dataframe from summarize_column_by_period\n","        should have business_id and review_count\n","    \n","    RETURN:\n","    regression_values: Pandas dataframe of business_id and r^2 value\n","    \"\"\"\n","    regression_values = []\n","    for business_id, new_df in dataframe.groupby(level=0):\n","        review_column = new_df.loc[:,'review_count']\n","        num_reviews = review_column.values\n","        x = num_reviews\n","        #print(num_reviews)\n","        y = np.arange(len(num_reviews))\n","        slope, intercept, r, p, se = ss.linregress(x, y)\n","        d = {'business_id': business_id,\n","             'slope': slope,\n","            'rvalue': r}\n","        regression_values.append(d)\n","    regression_values = pd.DataFrame(regression_values)\n","    return regression_values\n","  \n","\n","  # linear regression"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ucBhjHJg3tpW"},"source":["fig = plt.figure()\n","ax = fig.add_subplot(projection='3d')\n","x = regression_vals['rvalue']\n","y = regression_vals['slope']\n","hist, xedges, yedges = np.histogram2d(x, y, bins=5, range=[[-1, 1], [-2.25, 2.25]])\n","xpos, ypos = np.meshgrid(xedges[:-1] + 0.25, yedges[:-1] + 0.25, indexing=\"ij\")\n","xpos = xpos.ravel()\n","ypos = ypos.ravel()\n","zpos = 0\n","dx = dy = 0.5 * np.ones_like(zpos)\n","dz = hist.ravel()\n","plt.xlabel(\"R² value\")\n","plt.ylabel('Slope')\n","ax.set_zlabel('Number of businesses')\n","plt.title('Slope and R² value for each business, review count timeframe = 6 months')\n","\n","ax.bar3d(xpos, ypos, zpos, dx, dy, dz, shade = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YQE2o27P8Zen"},"source":["# Code below for chi-square analysis of review count coefficient and whether business is still open\n","summarized = summarize_column_by_period(preprocessed_review_delta, 'stars', timespan=6, unit='month')\n","trend = calculate_regression(summarized)\n","\n","decline_business_id = trend[trend['rvalue']<= -0.5]\n","decline_business_id.slope.mean()\n","decline_business = summarized[summarized.index.get_level_values(0).isin(decline_business_id['business_id'])]\n","decline = business_df[business_df['business_id'].isin(decline_business_id['business_id'])]\n","decline.is_open.value_counts()\n","\n","\n","increase_business_id = trend[trend['rvalue']>= 0.5]\n","increase_business_id.slope.mean()\n","increase_business = summarized[summarized.index.get_level_values(0).isin(increase_business_id['business_id'])]\n","increase = business_df[business_df['business_id'].isin(increase_business_id['business_id'])]\n","booming.is_open.value_counts()\n","\n","\n","crosstab = np.zeros((2,2))\n","crosstab[0, 0] = 2395\n","crosstab[0, 1] = 2594\n","crosstab[1, 0] = 867\n","crosstab[1, 1] = 301\n","chi2, p, dof, ex = ss.chi2_contingency(crosstab, correction=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-HSzZdxn8ZSx"},"source":["#Romir\n","df = pd.read_json(path+'yelp_academic_dataset_business.json', lines=True)\n","#dropna or won't iterate through nan values\n","df = df.dropna()\n","\n","#counting all subcategories\n","categories = []\n","for l in df[\"categories\"].dropna():\n","    cat_list = re.findall('[a-zA-Z]+', l)\n","    if(len(cat_list) > 1):\n","        categories.append(cat_list)\n","all_types = []\n","for lst in categories:\n","    for i in lst:\n","        if i not in all_types:\n","            all_types.append(i)\n","len(all_types)\n","\n","#adding new column to the dataframe to include main category\n","df[\"main\"] = \"main\"\n","#assigning correct value to main category\n","for index, row in df.iterrows():\n","    if(\"Active Life\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Active Life\"\n","    if(\"Arts & Entertainment\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Arts & Entertainment\"\n","    if(\"Automotive\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Automotive\"\n","    if(\"Beauty & Spas\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Beauty & Spas\"\n","    if(\"Education\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Education\"\n","    if(\"Event Planning & Services\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Event Planning & Services\"\n","    if(\"Financial Services\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Financial Services\"\n","    if(\"Food\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Food\"\n","    if(\"Health & Medical\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Health & Medical\"\n","    if(\"Home Services\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Home Services\"\n","    if(\"Hotels & Travel\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Hotels & Travel\"\n","    if(\"Local Flavor\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Local Flavor\"\n","    if(\"Local Services\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Local Services\"\n","    if(\"Mass Media\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Mass Media\"\n","    if(\"Nightlife\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Nightlife\"\n","    if(\"Pets\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Pets\"\n","    if(\"Professional Services\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Professional Services\"\n","    if(\"Public Services & Government\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Public Services & Government\"\n","    if(\"Real Estate\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Real Estate\"\n","    if(\"Religious Organizations\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Religious Organizations\"\n","    if(\"Restaurants\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Restaurants\"\n","    if(\"Shopping\" in row[\"categories\"]):\n","        df.at[index, \"main\"] = \"Shopping\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AMjAmJfBMBQi"},"source":["# I turned the code above into a function\n","# to use this function on a dataframe you would call:\n","# business_with_main_category = business_df.assign(main_category = assign_category(business_df['categories']))\n","\n","\n","def assign_category(lists_of_strings):\n","    \"\"\"\n","    lists_of_strings would be a list of strings\n","\n","    RETURN:\n","    output: the final main category it belongs to\n","    \"\"\"\n","    for i in lists_of_strings:\n","        if(\"Active Life\" in i):\n","            output = \"Active Life\"\n","        if(\"Arts & Entertainment\" in i):\n","            output = \"Arts & Entertainment\"\n","        if(\"Automotive\" in i):\n","            output = \"Automotive\"\n","        if(\"Beauty & Spas\" in i):\n","            output = \"Beauty & Spas\"\n","        if(\"Education\" in i):\n","            output = \"Education\"\n","        if(\"Event Planning & Services\" in i):\n","            output = \"Event Planning & Services\"\n","        if(\"Financial Services\" in i):\n","            output = \"Financial Services\"\n","        if(\"Food\" in i):\n","            output = \"Food\"\n","        if(\"Health & Medical\" in i):\n","            output = \"Health & Medical\"\n","        if(\"Home Services\" in i):\n","            output = \"Home Services\"\n","        if(\"Hotels & Travel\" in i):\n","            output = \"Hotels & Travel\"\n","        if(\"Local Flavor\" in i):\n","            output = \"Local Flavor\"\n","        if(\"Local Services\" in i):\n","            output = \"Local Services\"\n","        if(\"Mass Media\" in i):\n","            output = \"Mass Media\"\n","        if(\"Nightlife\" in i):\n","            output = \"Nightlife\"\n","        if(\"Pets\" in i):\n","            output = \"Pets\"\n","        if(\"Professional Services\" in i):\n","            output = \"Professional Services\"\n","        if(\"Public Services & Government\" in i):\n","            output = \"Public Services & Government\"\n","        if(\"Real Estate\" in i):\n","            output = \"Real Estate\"\n","        if(\"Religious Organizations\" in i):\n","            output = \"Religious Organizations\"\n","        if(\"Restaurants\" in i):\n","            output = \"Restaurants\"\n","        if(\"Shopping\" in i):\n","            output = \"Shopping\"\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vfothO7eJI5u"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JphW6sC48Y_X"},"source":["#boxplot of all business main categories and their review count\n","ax = sb.boxplot(x=df[\"main\"], y=df['review_count'])\n","ax.set_ylabel(\"Business Main Category\")\n","ax.set_xlabel(\"Number of Reviews\")\n","ax.set_title(\"Distribution of the number of review counts on business categories\")\n","sb.set(rc={'figure.figsize':(40,20)})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kujDl3rZrrld"},"source":["#data to be used for knn algorithm\n","main_cat = [\"Active Life\", \"Arts & Entertainment\", \"Automotive\", \"Food\", \"Beauty & Spas\", \"Education\",\"Event Planning & Services\", \"Financial Services\", \"Health & Medical\", \"Home Services\", \"Hotels & Travel\", \"Local Flavor\",\"Local Services\", \"Mass Media\", \"Nightlife\", \"Pets\", \"Professional Services\", \"Public Services & Government\", \"Real Estate\", \"Religious Organizations\", \"Restaurants\",\"Shopping\"]\n","review_count_mean = []\n","for cat in main_cat:\n","    review_count_mean.append(df[df[\"main\"] == cat][\"review_count\"].mean())\n","#make new dataframe with 2 columns: Main Category, Total Review Count (summing all businesses of the same main type)\n","data = pd.DataFrame(data={\"Main Category\": main_cat, \"Review Count Mean\": review_count_mean})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W6rarqKMr6b1"},"source":["#defining euclidean distance to be used in knn\n","def euclidean_distance(row):\n","    inner_value = 0\n","    for k in distance_columns:\n","        inner_value += (row[k] - selected_business[k]) ** 2\n","    return math.sqrt(inner_value)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uASqkX5lsF1R"},"source":["#add Closest Neighbor column to data\n","data[\"Closest Neighbor\"] = \"none\"\n","#perform knn for each business main category on data above\n","for index, row in data.iterrows():\n","    selected_business = data[data[\"Main Category\"] == row[\"Main Category\"]].iloc[0]\n","    distance_columns = [\"Review Count Mean\"]\n","    #distance of each business to other businesses\n","    business_distance = data.apply(euclidean_distance, axis=1)\n","    # Select only the numeric columns from the NBA dataset\n","    data_numeric = data[distance_columns]\n","    # Normalize all of the numeric columns\n","    data_normalized = (data_numeric - data_numeric.mean()) / data_numeric.std()\n","    \n","    data_normalized.fillna(0, inplace=True)\n","\n","    # Find the normalized vector for restaurants.\n","    business_normalized = data_normalized[data[\"Main Category\"] == row[\"Main Category\"]]\n","\n","    # Find the distance between selected business and everyone else.\n","    euclidean_distances = data_normalized.apply(lambda row: distance.euclidean(row, business_normalized), axis=1)\n","\n","    # Create a new dataframe with distances.\n","    distance_frame = pd.DataFrame(data={\"dist\": euclidean_distances, \"idx\": euclidean_distances.index})\n","    distance_frame.sort_values(\"dist\", inplace=True)\n","    # Find the most similar business to selected business (the lowest distance to selected_business is itself, second closest is second_smallest)\n","    second_smallest = distance_frame.iloc[1][\"idx\"]\n","    most_similar_to_business = data.loc[int(second_smallest)][\"Main Category\"]\n","    #assign closest neighbor\n","    data.at[index, \"Closest Neighbor\"] = most_similar_to_business"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YnQO9y2lsedO"},"source":["#check the dataframe to see closest neighbors of each categories\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t-a3HezFQ0OM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nPsyJDfwQ0eF"},"source":["### Project Presentation ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D5iqkUPlQ3E0"},"source":["#Cleaning review data - only keeping reviews made in English\n","\n","preprocessed_review_delta[preprocessed_review_delta['text'].map(lambda x: x.isascii())]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C5qcVw2nUELW"},"source":["import seaborn as sns\n","sns.lineplot(x='date', y='count', hue = 'group', data = daily_summary, palette=\"flare\", legend=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JXX8F9ebWENO"},"source":["#df reviews\n","#add column that sums up votes of type (useful + funny + cool) to: total_votes\n","df[\"total_votes\"] = df[\"useful\"] + df[\"funny\"] + df[\"cool\"]\n","#mean of total votes is about 2.17\n","df_built[\"total_votes\"].mean()\n","#top 5% of votes \n","top_pct = df[df[\"total_votes\"] >= df[\"total_votes\"].quantile(0.95)].reset_index()\n","#if 10% -> 11; if 5% -> 19.13\n","top_pct[\"total_votes\"].mean()\n","\n","#add sentiment column\n","df[\"sentiment\"] = df[[\"text\"]].applymap(lambda x: TextBlob(x).sentiment.polarity)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aWGOYn8WWEF1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"83KCy_YmWD5i"},"source":["## code to generate features for machine learning model\n","# 'cleaned_review.pkl' is what I saved after running Sophia's one line to drop all the unintepretable reviews\n","# should contain\n","reviews_df = pd.read_pickle('cleaned_review.pkl') # this is the main dataframe\n","\n","reviews_df['total_votes'] = reviews_df[['useful', 'funny', 'cool']].sum(axis=1)\n","reviews = reviews_df.reset_index().set_index('review_id')\n","reviews.date = pd.to_datetime(reviews.date)\n","\n","# get average ratings for the previous 6m of the review\n","shift_stars = reviews[['stars']].shift(1)\n","reviews_subset = reviews[['business_id', 'date']].join(shift_stars)\n","rolling_stars = reviews_subset.groupby('business_id').rolling('180D', on='date').mean()\n","    # since this value for the \"first\" review is always going to be empty\n","    # fill it with the average review of the restaurant across the entire time\n","rolling_stars['stars'] = rolling_stars[['stars']].groupby('business_id').transform(lambda x: x.fillna(x.mean()))\n","\n","rolling_stars = rolling_stars.reset_index()\n","reviews_df = reviews_df.reset_index()\n","\n","reviews_df['prev6M_rating'] = rolling_stars['stars']\n","\n","# get total count of reviews for previous 6m\n","temp = rolling_stars.groupby('business_id').rolling('180D', on='date').count()\n","reviews_df['prev6M_count'] = temp.drop(columns='business_id').reset_index().stars - 1\n","\n","## code to generate the target for machine learning model\n","# the target is projected rating for the next 6 months\n","# summarize based on a forward-looking window\n","subset1 = reviews_df[['business_id', 'date', 'review_id', 'stars']]\n","subset1['date'] = pd.to_datetime(subset1['date'])\n","subset1 = subset1.set_index(['business_id', 'date', 'review_id'])\n","# Ended up using a rather crude way, looking 20 rows ahead at a time\n","# doing it based on actual date takes a long time - but can be worked on in the future\n","indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=20)\n","subset1_forward = subset1.groupby('business_id').rolling(window=indexer, min_periods=1).mean()\n","\n","reviews_df['projected6M_rating'] = subset1_forward.reset_index().stars\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7ZX85iZctBm"},"source":["# Load the pickle containing the sentiment analyses results as reviews_df:\n","# seperate out the NLTK sentiment vector result into four columns\n","reviews_df['compound'] = reviews_df[['polarity_score']].applymap(lambda x: x['compound'], na_action='ignore')\n","reviews_df['neg'] = reviews_df[['polarity_score']].applymap(lambda x: x['neg'], na_action='ignore')\n","reviews_df['neu'] = reviews_df[['polarity_score']].applymap(lambda x: x['neu'], na_action='ignore')\n","reviews_df['pos'] = reviews_df[['polarity_score']].applymap(lambda x: x['pos'], na_action='ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3QM_8UANcs2w"},"source":["### Machine learning model\n","training_data = reviews_df[['business_id', 'total_votes', 'prev6M_rating', 'prev6M_count', 'compound',\n","                           'neg', 'neu', 'pos']]\n","testing_data = reviews_df[['projected6M_rating']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5-y-cOracsws"},"source":["from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.model_selection import train_test_split\n","# need to label businesses\n","from sklearn import preprocessing\n","\n","le = preprocessing.LabelEncoder()\n","le.fit(reviews_df['business_id'])\n","reviews_df['business_encoded'] = le.transform(reviews_df['business_id'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PNeXgCBx8JG7"},"source":["# on another thought, we prob don't need to include business_encoded / business_id as a feature\n","training_data = reviews_df[['business_encoded', 'total_votes', 'prev6M_rating', 'prev6M_count', 'compound',\n","                           'neg', 'neu', 'pos']]\n","\n","testing_data = reviews_df['projected6M_rating'].values\n","\n","X_train, X_test, y_train, y_test = train_test_split(training_data, testing_data, random_state=0)\n","\n","reg = GradientBoostingRegressor()\n","reg.fit(X_train, y_train)\n","reg.score(X_test, y_test)\n","\n","reg.feature_importances_\n","plt.bar(x =np.arange(0, 8) ,height =reg.feature_importances_, tick_label=['total_votes', 'prev6M_rating', 'prev6M_count', 'compound',\n","                           'neg', 'neu', 'pos'])"],"execution_count":null,"outputs":[]}]}